{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4561d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchaudio\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import ASTForAudioClassification, AutoProcessor\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35d5d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ravdess = \"/mnt/c/Users/rubom/Desktop/Emotions DataSets/Ravdess/\"\n",
    "Crema   = \"/mnt/c/Users/rubom/Desktop/Emotions DataSets/Crema/AudioWAV/\"\n",
    "Tess    = \"/mnt/c/Users/rubom/Desktop/Emotions DataSets/Tess/TESS Toronto emotional speech set data/\"\n",
    "Savee   = \"/mnt/c/Users/rubom/Desktop/Emotions DataSets/Savee/ALL/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d6128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Emotions                                               Path\n",
      "0  neutral  /mnt/c/Users/rubom/Desktop/Emotions DataSets/R...\n",
      "1  neutral  /mnt/c/Users/rubom/Desktop/Emotions DataSets/R...\n",
      "2  neutral  /mnt/c/Users/rubom/Desktop/Emotions DataSets/R...\n",
      "3  neutral  /mnt/c/Users/rubom/Desktop/Emotions DataSets/R...\n",
      "4     calm  /mnt/c/Users/rubom/Desktop/Emotions DataSets/R...\n",
      "/mnt/c/Users/rubom/Desktop/Emotions DataSets/Ravdess/Actor_01/03-01-01-01-01-02-01.wav\n"
     ]
    }
   ],
   "source": [
    "ravdess_directory_list = [d for d in os.listdir(Ravdess) if d.startswith(\"Actor_\")]\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for dir in ravdess_directory_list:\n",
    "    actor_folder = os.path.join(Ravdess, dir)\n",
    "    \n",
    "    for file in os.listdir(actor_folder):\n",
    "        if not file.endswith('.wav'):\n",
    "            continue  \n",
    "        \n",
    "        part = file.split('.')[0].split('-')\n",
    "        \n",
    "        if len(part) != 7:\n",
    "            print(f\"⚠️ Skipping malformed file: {file}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            emotion_code = int(part[2])\n",
    "            file_emotion.append(emotion_code)\n",
    "            file_path.append(os.path.join(actor_folder, file))\n",
    "        except ValueError:\n",
    "            print(f\"❌ Couldn't parse emotion code in: {file}\")\n",
    "\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Ravdess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "\n",
    "emotion_labels = {\n",
    "    1: 'neutral',\n",
    "    2: 'calm',\n",
    "    3: 'happy',\n",
    "    4: 'sad',\n",
    "    5: 'angry',\n",
    "    6: 'fear',\n",
    "    7: 'disgust',\n",
    "    8: 'surprise'\n",
    "}\n",
    "Ravdess_df[\"Emotions\"] = Ravdess_df[\"Emotions\"].map(emotion_labels)\n",
    "\n",
    "print(Ravdess_df.head())\n",
    "print(Ravdess_df[\"Path\"].values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a913cea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/rubom/Desktop/Emotions DataSets/Crema/AudioWAV/1001_DFA_DIS_XX.wav\n"
     ]
    }
   ],
   "source": [
    "crema_directory_list = os.listdir(Crema)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for file in crema_directory_list:\n",
    "\n",
    "    file_path.append(Crema + file)\n",
    "\n",
    "    part=file.split('_')\n",
    "    if part[2] == 'SAD':\n",
    "        file_emotion.append('sad')\n",
    "    elif part[2] == 'ANG':\n",
    "        file_emotion.append('angry')\n",
    "    elif part[2] == 'DIS':\n",
    "        file_emotion.append('disgust')\n",
    "    elif part[2] == 'FEA':\n",
    "        file_emotion.append('fear')\n",
    "    elif part[2] == 'HAP':\n",
    "        file_emotion.append('happy')\n",
    "    elif part[2] == 'NEU':\n",
    "        file_emotion.append('neutral')\n",
    "    else:\n",
    "        file_emotion.append('Unknown')\n",
    "        \n",
    "\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Crema_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Crema_df.head()\n",
    "print(Crema_df[\"Path\"].values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed964004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/rubom/Desktop/Emotions DataSets/Tess/TESS Toronto emotional speech set data/OAF_angry/OAF_bar_angry.wav\n"
     ]
    }
   ],
   "source": [
    "tess_directory_list = os.listdir(Tess)\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for dir in tess_directory_list:\n",
    "    directories = os.listdir(Tess + \"/\"+dir)\n",
    "    for file in directories:\n",
    "        part = file.split('.')[0]\n",
    "        part = part.split('_')[2]\n",
    "\n",
    "        if part=='ps':\n",
    "            file_emotion.append('surprise')\n",
    "        else:\n",
    "            file_emotion.append(part)\n",
    "        file_path.append(Tess + dir + '/' + file)\n",
    "        \n",
    "\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Tess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Tess_df.head()\n",
    "print(Tess_df['Path'].values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720cdecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/rubom/Desktop/Emotions DataSets/Savee/ALL/DC_a02.wav\n"
     ]
    }
   ],
   "source": [
    "savee_directory_list = os.listdir(Savee)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for file in savee_directory_list:\n",
    "    file_path.append(Savee + file)\n",
    "    part = file.split('_')[1]\n",
    "    ele = part[:-6]\n",
    "    if ele=='a':\n",
    "        file_emotion.append('angry')\n",
    "    elif ele=='d':\n",
    "        file_emotion.append('disgust')\n",
    "    elif ele=='f':\n",
    "        file_emotion.append('fear')\n",
    "    elif ele=='h':\n",
    "        file_emotion.append('happy')\n",
    "    elif ele=='n':\n",
    "        file_emotion.append('neutral')\n",
    "    elif ele=='sa':\n",
    "        file_emotion.append('sad')\n",
    "    else:\n",
    "        file_emotion.append('surprise')\n",
    "        \n",
    "\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Savee_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Savee_df.head()\n",
    "print(Savee_df[\"Path\"].values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eb9bcea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>/mnt/c/Users/rubom/Desktop/Emotions DataSets/R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>/mnt/c/Users/rubom/Desktop/Emotions DataSets/R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>/mnt/c/Users/rubom/Desktop/Emotions DataSets/R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>/mnt/c/Users/rubom/Desktop/Emotions DataSets/R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>calm</td>\n",
       "      <td>/mnt/c/Users/rubom/Desktop/Emotions DataSets/R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>surprise</td>\n",
       "      <td>/mnt/c/Users/rubom/Desktop/Emotions DataSets/S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>surprise</td>\n",
       "      <td>/mnt/c/Users/rubom/Desktop/Emotions DataSets/S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>surprise</td>\n",
       "      <td>/mnt/c/Users/rubom/Desktop/Emotions DataSets/S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>surprise</td>\n",
       "      <td>/mnt/c/Users/rubom/Desktop/Emotions DataSets/S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>surprise</td>\n",
       "      <td>/mnt/c/Users/rubom/Desktop/Emotions DataSets/S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12162 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Emotions                                               Path\n",
       "0     neutral  /mnt/c/Users/rubom/Desktop/Emotions DataSets/R...\n",
       "1     neutral  /mnt/c/Users/rubom/Desktop/Emotions DataSets/R...\n",
       "2     neutral  /mnt/c/Users/rubom/Desktop/Emotions DataSets/R...\n",
       "3     neutral  /mnt/c/Users/rubom/Desktop/Emotions DataSets/R...\n",
       "4        calm  /mnt/c/Users/rubom/Desktop/Emotions DataSets/R...\n",
       "..        ...                                                ...\n",
       "475  surprise  /mnt/c/Users/rubom/Desktop/Emotions DataSets/S...\n",
       "476  surprise  /mnt/c/Users/rubom/Desktop/Emotions DataSets/S...\n",
       "477  surprise  /mnt/c/Users/rubom/Desktop/Emotions DataSets/S...\n",
       "478  surprise  /mnt/c/Users/rubom/Desktop/Emotions DataSets/S...\n",
       "479  surprise  /mnt/c/Users/rubom/Desktop/Emotions DataSets/S...\n",
       "\n",
       "[12162 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = pd.concat([Ravdess_df, Crema_df, Tess_df, Savee_df], axis = 0)\n",
    "data_path.to_csv(\"data_path.csv\",index=False)\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f09c6586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Count of Emotions', size=16)\n",
    "sns.countplot(data_path.Emotions)\n",
    "plt.ylabel('Count', size=12)\n",
    "plt.xlabel('Emotions', size=12)\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b86bb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-speech-commands-v2 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([35]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([35, 768]) in the checkpoint and torch.Size([8, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/75\n",
      "\n",
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.9332, Accuracy: 25.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raf/miniconda3/envs/tf/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raf/miniconda3/envs/tf/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raf/miniconda3/envs/tf/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raf/miniconda3/envs/tf/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raf/miniconda3/envs/tf/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raf/miniconda3/envs/tf/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.8863, Accuracy: 34.00%\n",
      "F1 Score: 0.2926\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.3999    0.5410    0.4599      2695\n",
      "        calm     0.1740    0.9323    0.2933       266\n",
      "     disgust     0.0000    0.0000    0.0000      2695\n",
      "        fear     0.3572    0.3280    0.3420      2695\n",
      "       happy     0.2494    0.0827    0.1243      2695\n",
      "     neutral     0.3332    0.5424    0.4128      2380\n",
      "         sad     0.3655    0.3737    0.3695      2695\n",
      "    surprise     0.3468    0.7473    0.4737       910\n",
      "\n",
      "    accuracy                         0.3400     17031\n",
      "   macro avg     0.2782    0.4434    0.3094     17031\n",
      "weighted avg     0.2849    0.3400    0.2926     17031\n",
      "\n",
      "✅ Checkpoint saved at epoch 1\n",
      "\n",
      "Epoch 2/75\n",
      "\n",
      "Epoch 2/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 1.8257, Accuracy: 36.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.8311, Accuracy: 39.41%\n",
      "F1 Score: 0.3578\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.4866    0.6011    0.5378      2695\n",
      "        calm     0.2002    0.9699    0.3318       266\n",
      "     disgust     0.6768    0.0412    0.0776      2695\n",
      "        fear     0.5073    0.2583    0.3423      2695\n",
      "       happy     0.3271    0.2212    0.2639      2695\n",
      "     neutral     0.3626    0.5542    0.4384      2380\n",
      "         sad     0.3909    0.5098    0.4425      2695\n",
      "    surprise     0.3880    0.8110    0.5249       910\n",
      "\n",
      "    accuracy                         0.3941     17031\n",
      "   macro avg     0.4174    0.4958    0.3699     17031\n",
      "weighted avg     0.4525    0.3941    0.3578     17031\n",
      "\n",
      "✅ Checkpoint saved at epoch 2\n",
      "\n",
      "Epoch 3/75\n",
      "\n",
      "Epoch 3/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 1.7857, Accuracy: 41.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.7999, Accuracy: 43.36%\n",
      "F1 Score: 0.4036\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.4904    0.7109    0.5804      2695\n",
      "        calm     0.2013    0.9662    0.3331       266\n",
      "     disgust     0.7367    0.1340    0.2267      2695\n",
      "        fear     0.4729    0.3039    0.3700      2695\n",
      "       happy     0.3726    0.1844    0.2467      2695\n",
      "     neutral     0.4270    0.5508    0.4811      2380\n",
      "         sad     0.4211    0.5547    0.4788      2695\n",
      "    surprise     0.4357    0.8000    0.5641       910\n",
      "\n",
      "    accuracy                         0.4336     17031\n",
      "   macro avg     0.4447    0.5256    0.4101     17031\n",
      "weighted avg     0.4807    0.4336    0.4036     17031\n",
      "\n",
      "✅ Checkpoint saved at epoch 3\n",
      "\n",
      "Epoch 4/75\n",
      "\n",
      "Epoch 4/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 1.7622, Accuracy: 43.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.7807, Accuracy: 44.98%\n",
      "F1 Score: 0.4291\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.5621    0.6516    0.6035      2695\n",
      "        calm     0.2334    0.9662    0.3760       266\n",
      "     disgust     0.7108    0.1532    0.2521      2695\n",
      "        fear     0.5530    0.2827    0.3742      2695\n",
      "       happy     0.3547    0.3143    0.3333      2695\n",
      "     neutral     0.4335    0.5571    0.4876      2380\n",
      "         sad     0.4438    0.5748    0.5009      2695\n",
      "    surprise     0.3932    0.8253    0.5326       910\n",
      "\n",
      "    accuracy                         0.4498     17031\n",
      "   macro avg     0.4606    0.5407    0.4325     17031\n",
      "weighted avg     0.5005    0.4498    0.4291     17031\n",
      "\n",
      "✅ Checkpoint saved at epoch 4\n",
      "\n",
      "Epoch 5/75\n",
      "\n",
      "Epoch 5/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 1.7439, Accuracy: 45.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.7680, Accuracy: 46.38%\n",
      "F1 Score: 0.4474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.5814    0.6571    0.6170      2695\n",
      "        calm     0.2180    0.9737    0.3563       266\n",
      "     disgust     0.5926    0.2126    0.3129      2695\n",
      "        fear     0.5625    0.2720    0.3667      2695\n",
      "       happy     0.3887    0.3150    0.3480      2695\n",
      "     neutral     0.4407    0.5992    0.5078      2380\n",
      "         sad     0.4588    0.5718    0.5091      2695\n",
      "    surprise     0.4273    0.8209    0.5621       910\n",
      "\n",
      "    accuracy                         0.4638     17031\n",
      "   macro avg     0.4588    0.5528    0.4475     17031\n",
      "weighted avg     0.4967    0.4638    0.4474     17031\n",
      "\n",
      "✅ Checkpoint saved at epoch 5\n",
      "\n",
      "Epoch 6/75\n",
      "🔓 Unfreezing AST backbone...\n",
      "\n",
      "Epoch 6/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 1.6533, Accuracy: 55.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6780, Accuracy: 55.49%\n",
      "F1 Score: 0.5468\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.6940    0.7592    0.7251      2695\n",
      "        calm     0.1965    1.0000    0.3284       266\n",
      "     disgust     0.7634    0.2909    0.4213      2695\n",
      "        fear     0.6922    0.4263    0.5277      2695\n",
      "       happy     0.5709    0.3614    0.4426      2695\n",
      "     neutral     0.4614    0.8101    0.5879      2380\n",
      "         sad     0.5855    0.5562    0.5705      2695\n",
      "    surprise     0.5034    0.8835    0.6414       910\n",
      "\n",
      "    accuracy                         0.5549     17031\n",
      "   macro avg     0.5584    0.6360    0.5306     17031\n",
      "weighted avg     0.6176    0.5549    0.5468     17031\n",
      "\n",
      "✅ Checkpoint saved at epoch 6\n",
      "\n",
      "Epoch 7/75\n",
      "\n",
      "Epoch 7/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 1.5558, Accuracy: 66.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6375, Accuracy: 60.55%\n",
      "F1 Score: 0.6071\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.7708    0.7039    0.7358      2695\n",
      "        calm     0.2530    0.9474    0.3994       266\n",
      "     disgust     0.6245    0.4393    0.5158      2695\n",
      "        fear     0.5872    0.5161    0.5494      2695\n",
      "       happy     0.5562    0.6078    0.5809      2695\n",
      "     neutral     0.6619    0.6458    0.6538      2380\n",
      "         sad     0.6043    0.5989    0.6016      2695\n",
      "    surprise     0.5828    0.8780    0.7006       910\n",
      "\n",
      "    accuracy                         0.6055     17031\n",
      "   macro avg     0.5801    0.6672    0.5921     17031\n",
      "weighted avg     0.6249    0.6055    0.6071     17031\n",
      "\n",
      "✅ Checkpoint saved at epoch 7\n",
      "\n",
      "Epoch 8/75\n",
      "\n",
      "Epoch 8/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 1.5000, Accuracy: 73.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6331, Accuracy: 61.57%\n",
      "F1 Score: 0.6146\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.7216    0.7673    0.7438      2695\n",
      "        calm     0.3242    0.9323    0.4811       266\n",
      "     disgust     0.6047    0.4813    0.5360      2695\n",
      "        fear     0.5997    0.4998    0.5452      2695\n",
      "       happy     0.6164    0.5599    0.5868      2695\n",
      "     neutral     0.5789    0.7029    0.6349      2380\n",
      "         sad     0.5848    0.6104    0.5973      2695\n",
      "    surprise     0.8147    0.7681    0.7907       910\n",
      "\n",
      "    accuracy                         0.6157     17031\n",
      "   macro avg     0.6056    0.6653    0.6145     17031\n",
      "weighted avg     0.6243    0.6157    0.6146     17031\n",
      "\n",
      "✅ Checkpoint saved at epoch 8\n",
      "\n",
      "Epoch 9/75\n",
      "\n",
      "Epoch 9/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 1.4657, Accuracy: 77.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6351, Accuracy: 61.16%\n",
      "F1 Score: 0.6077\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.7236    0.7596    0.7411      2695\n",
      "        calm     0.3279    0.9023    0.4810       266\n",
      "     disgust     0.5267    0.5570    0.5414      2695\n",
      "        fear     0.7100    0.3889    0.5025      2695\n",
      "       happy     0.6429    0.5317    0.5820      2695\n",
      "     neutral     0.5949    0.7084    0.6467      2380\n",
      "         sad     0.5670    0.6278    0.5959      2695\n",
      "    surprise     0.7010    0.8451    0.7663       910\n",
      "\n",
      "    accuracy                         0.6116     17031\n",
      "   macro avg     0.5992    0.6651    0.6071     17031\n",
      "weighted avg     0.6274    0.6116    0.6077     17031\n",
      "\n",
      "✅ Checkpoint saved at epoch 9\n",
      "\n",
      "Epoch 10/75\n",
      "\n",
      "Epoch 10/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1.4437, Accuracy: 80.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6273, Accuracy: 61.68%\n",
      "F1 Score: 0.6149\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.7108    0.7751    0.7416      2695\n",
      "        calm     0.2979    0.9286    0.4511       266\n",
      "     disgust     0.6394    0.4705    0.5421      2695\n",
      "        fear     0.6311    0.4939    0.5541      2695\n",
      "       happy     0.5837    0.5744    0.5790      2695\n",
      "     neutral     0.6204    0.6887    0.6527      2380\n",
      "         sad     0.5871    0.6015    0.5942      2695\n",
      "    surprise     0.6819    0.8363    0.7512       910\n",
      "\n",
      "    accuracy                         0.6168     17031\n",
      "   macro avg     0.5940    0.6711    0.6083     17031\n",
      "weighted avg     0.6266    0.6168    0.6149     17031\n",
      "\n",
      "✅ Checkpoint saved at epoch 10\n",
      "\n",
      "Epoch 11/75\n",
      "\n",
      "Epoch 11/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 1.4285, Accuracy: 81.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6340, Accuracy: 61.36%\n",
      "F1 Score: 0.6107\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.6693    0.7944    0.7265      2695\n",
      "        calm     0.2999    0.9323    0.4538       266\n",
      "     disgust     0.6655    0.4304    0.5228      2695\n",
      "        fear     0.5546    0.5803    0.5672      2695\n",
      "       happy     0.6456    0.5069    0.5679      2695\n",
      "     neutral     0.6316    0.6786    0.6542      2380\n",
      "         sad     0.5920    0.5981    0.5951      2695\n",
      "    surprise     0.7122    0.8187    0.7618       910\n",
      "\n",
      "    accuracy                         0.6136     17031\n",
      "   macro avg     0.5963    0.6675    0.6061     17031\n",
      "weighted avg     0.6258    0.6136    0.6107     17031\n",
      "\n",
      "✅ Checkpoint saved at epoch 11\n",
      "\n",
      "Epoch 12/75\n",
      "\n",
      "Epoch 12/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 1.4166, Accuracy: 83.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6374, Accuracy: 62.12%\n",
      "F1 Score: 0.6170\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.6067    0.8442    0.7060      2695\n",
      "        calm     0.3545    0.6917    0.4688       266\n",
      "     disgust     0.6232    0.5106    0.5613      2695\n",
      "        fear     0.5905    0.5510    0.5701      2695\n",
      "       happy     0.6493    0.5091    0.5707      2695\n",
      "     neutral     0.6241    0.6773    0.6496      2380\n",
      "         sad     0.6521    0.5696    0.6080      2695\n",
      "    surprise     0.7482    0.8132    0.7794       910\n",
      "\n",
      "    accuracy                         0.6212     17031\n",
      "   macro avg     0.6061    0.6458    0.6142     17031\n",
      "weighted avg     0.6267    0.6212    0.6170     17031\n",
      "\n",
      "✅ Checkpoint saved at epoch 12\n",
      "\n",
      "Epoch 13/75\n",
      "\n",
      "Epoch 13/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 1.4058, Accuracy: 84.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6250, Accuracy: 62.93%\n",
      "F1 Score: 0.6278\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.7280    0.7447    0.7362      2695\n",
      "        calm     0.3779    0.8609    0.5252       266\n",
      "     disgust     0.6231    0.5043    0.5574      2695\n",
      "        fear     0.6331    0.4731    0.5415      2695\n",
      "       happy     0.6001    0.6204    0.6101      2695\n",
      "     neutral     0.6625    0.6748    0.6686      2380\n",
      "         sad     0.5477    0.6876    0.6097      2695\n",
      "    surprise     0.8136    0.7868    0.8000       910\n",
      "\n",
      "    accuracy                         0.6293     17031\n",
      "   macro avg     0.6233    0.6691    0.6311     17031\n",
      "weighted avg     0.6376    0.6293    0.6278     17031\n",
      "\n",
      "✅ Checkpoint saved at epoch 13\n",
      "\n",
      "Epoch 14/75\n",
      "\n",
      "Epoch 14/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 1.3970, Accuracy: 85.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6365, Accuracy: 61.04%\n",
      "F1 Score: 0.6077\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.6960    0.7536    0.7237      2695\n",
      "        calm     0.3792    0.8910    0.5320       266\n",
      "     disgust     0.6275    0.4712    0.5382      2695\n",
      "        fear     0.5745    0.4865    0.5268      2695\n",
      "       happy     0.5452    0.6241    0.5820      2695\n",
      "     neutral     0.7079    0.5887    0.6428      2380\n",
      "         sad     0.5973    0.6263    0.6115      2695\n",
      "    surprise     0.6006    0.8527    0.7048       910\n",
      "\n",
      "    accuracy                         0.6104     17031\n",
      "   macro avg     0.5910    0.6618    0.6077     17031\n",
      "weighted avg     0.6181    0.6104    0.6077     17031\n",
      "\n",
      "✅ Checkpoint saved at epoch 14\n",
      "\n",
      "Epoch 15/75\n",
      "\n",
      "Epoch 15/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 1.3899, Accuracy: 86.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6220, Accuracy: 62.57%\n",
      "F1 Score: 0.6246\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.7402    0.7243    0.7322      2695\n",
      "        calm     0.3522    0.8872    0.5043       266\n",
      "     disgust     0.6053    0.5087    0.5528      2695\n",
      "        fear     0.6075    0.5380    0.5706      2695\n",
      "       happy     0.6030    0.6030    0.6030      2695\n",
      "     neutral     0.6078    0.7143    0.6568      2380\n",
      "         sad     0.6218    0.5759    0.5980      2695\n",
      "    surprise     0.7103    0.8462    0.7723       910\n",
      "\n",
      "    accuracy                         0.6257     17031\n",
      "   macro avg     0.6060    0.6747    0.6237     17031\n",
      "weighted avg     0.6312    0.6257    0.6246     17031\n",
      "\n",
      "✅ Checkpoint saved at epoch 15\n",
      "\n",
      "Epoch 16/75\n",
      "\n",
      "Epoch 16/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 1.3856, Accuracy: 87.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6266, Accuracy: 62.49%\n",
      "F1 Score: 0.6218\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.6434    0.8215    0.7216      2695\n",
      "        calm     0.3710    0.7782    0.5024       266\n",
      "     disgust     0.5869    0.4935    0.5362      2695\n",
      "        fear     0.5705    0.5662    0.5683      2695\n",
      "       happy     0.6232    0.5481    0.5832      2695\n",
      "     neutral     0.6824    0.6634    0.6728      2380\n",
      "         sad     0.6468    0.5763    0.6095      2695\n",
      "    surprise     0.7525    0.8319    0.7902       910\n",
      "\n",
      "    accuracy                         0.6249     17031\n",
      "   macro avg     0.6096    0.6599    0.6230     17031\n",
      "weighted avg     0.6273    0.6249    0.6218     17031\n",
      "\n",
      "✅ Checkpoint saved at epoch 16\n",
      "\n",
      "Epoch 17/75\n",
      "\n",
      "Epoch 17/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 1.3785, Accuracy: 87.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6269, Accuracy: 62.57%\n",
      "F1 Score: 0.6224\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.6956    0.7529    0.7231      2695\n",
      "        calm     0.3651    0.7932    0.5000       266\n",
      "     disgust     0.6548    0.4490    0.5327      2695\n",
      "        fear     0.6051    0.5299    0.5650      2695\n",
      "       happy     0.6099    0.5848    0.5971      2695\n",
      "     neutral     0.5943    0.7336    0.6566      2380\n",
      "         sad     0.6001    0.6304    0.6149      2695\n",
      "    surprise     0.7764    0.8319    0.8032       910\n",
      "\n",
      "    accuracy                         0.6257     17031\n",
      "   macro avg     0.6127    0.6632    0.6241     17031\n",
      "weighted avg     0.6311    0.6257    0.6224     17031\n",
      "\n",
      "✅ Checkpoint saved at epoch 17\n",
      "\n",
      "Epoch 18/75\n",
      "\n",
      "Epoch 18/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: 1.3749, Accuracy: 88.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6262, Accuracy: 62.94%\n",
      "F1 Score: 0.6269\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.6847    0.7766    0.7277      2695\n",
      "        calm     0.3654    0.7707    0.4958       266\n",
      "     disgust     0.6252    0.4902    0.5495      2695\n",
      "        fear     0.5952    0.5325    0.5621      2695\n",
      "       happy     0.6491    0.5655    0.6044      2695\n",
      "     neutral     0.6478    0.6668    0.6571      2380\n",
      "         sad     0.5837    0.6705    0.6241      2695\n",
      "    surprise     0.7508    0.8209    0.7843       910\n",
      "\n",
      "    accuracy                         0.6294     17031\n",
      "   macro avg     0.6127    0.6617    0.6256     17031\n",
      "weighted avg     0.6329    0.6294    0.6269     17031\n",
      "\n",
      "✅ Checkpoint saved at epoch 18\n",
      "\n",
      "Epoch 19/75\n",
      "\n",
      "Epoch 19/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: 1.3710, Accuracy: 88.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6311, Accuracy: 62.15%\n",
      "F1 Score: 0.6192\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.7261    0.7525    0.7391      2695\n",
      "        calm     0.3792    0.8496    0.5244       266\n",
      "     disgust     0.6305    0.4490    0.5245      2695\n",
      "        fear     0.6037    0.4816    0.5358      2695\n",
      "       happy     0.5851    0.6505    0.6161      2695\n",
      "     neutral     0.7030    0.6265    0.6625      2380\n",
      "         sad     0.5267    0.6813    0.5941      2695\n",
      "    surprise     0.7649    0.8154    0.7894       910\n",
      "\n",
      "    accuracy                         0.6215     17031\n",
      "   macro avg     0.6149    0.6633    0.6232     17031\n",
      "weighted avg     0.6312    0.6215    0.6192     17031\n",
      "\n",
      "✅ Checkpoint saved at epoch 19\n",
      "\n",
      "Epoch 20/75\n",
      "\n",
      "Epoch 20/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 1.3689, Accuracy: 89.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6267, Accuracy: 62.67%\n",
      "F1 Score: 0.6256\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.7304    0.7570    0.7434      2695\n",
      "        calm     0.3886    0.8459    0.5325       266\n",
      "     disgust     0.6275    0.4712    0.5382      2695\n",
      "        fear     0.5947    0.5429    0.5676      2695\n",
      "       happy     0.6019    0.6007    0.6013      2695\n",
      "     neutral     0.7085    0.6076    0.6542      2380\n",
      "         sad     0.5394    0.6987    0.6088      2695\n",
      "    surprise     0.7629    0.7989    0.7805       910\n",
      "\n",
      "    accuracy                         0.6267     17031\n",
      "   macro avg     0.6192    0.6654    0.6283     17031\n",
      "weighted avg     0.6354    0.6267    0.6256     17031\n",
      "\n",
      "✅ Checkpoint saved at epoch 20\n",
      "\n",
      "Epoch 21/75\n",
      "\n",
      "Epoch 21/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Loss: 1.3630, Accuracy: 89.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6308, Accuracy: 62.53%\n",
      "F1 Score: 0.6231\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.7708    0.7076    0.7379      2695\n",
      "        calm     0.4518    0.7218    0.5557       266\n",
      "     disgust     0.6723    0.4263    0.5218      2695\n",
      "        fear     0.5489    0.5688    0.5587      2695\n",
      "       happy     0.5659    0.6512    0.6056      2695\n",
      "     neutral     0.6334    0.6933    0.6620      2380\n",
      "         sad     0.5904    0.6364    0.6125      2695\n",
      "    surprise     0.7341    0.8220    0.7755       910\n",
      "\n",
      "    accuracy                         0.6253     17031\n",
      "   macro avg     0.6209    0.6534    0.6287     17031\n",
      "weighted avg     0.6330    0.6253    0.6231     17031\n",
      "\n",
      "✅ Checkpoint saved at epoch 21\n",
      "\n",
      "Epoch 22/75\n",
      "\n",
      "Epoch 22/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Loss: 1.3625, Accuracy: 89.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  16%|███████▋                                         | 83/533 [00:46<03:53,  1.93it/s, acc=63.3, loss=1.89]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def volume_perturbation(data, low=0.5, high=1.5):\n",
    "    factor = np.random.uniform(low, high)\n",
    "    return data * factor\n",
    "\n",
    "def clipping_distortion(data, clip_percent=0.02):\n",
    "    threshold = clip_percent * np.max(np.abs(data))\n",
    "    data = np.clip(data, -threshold, threshold)\n",
    "    return data\n",
    "\n",
    "def pink_noise(data):\n",
    "    # Voss-McCartney algorithm-based pink noise\n",
    "    uneven = data.shape[0] % 2\n",
    "    X = np.random.randn(data.shape[0]//2 + 1 + uneven) + 1j * np.random.randn(data.shape[0]//2 + 1 + uneven)\n",
    "    S = np.sqrt(np.arange(len(X)) + 1.)  # Spectral shaping\n",
    "    y = (np.fft.irfft(X / S)).real\n",
    "    if uneven:\n",
    "        y = y[:-1]\n",
    "    y = y / np.max(np.abs(y))  # Normalize\n",
    "    return data + 0.005 * y\n",
    "\n",
    "def salt_and_pepper_audio(data, amount=0.005):\n",
    "    noisy_data = data.copy()\n",
    "    num_samples = int(amount * data.shape[0])\n",
    "    indices = np.random.randint(0, data.shape[0], num_samples)\n",
    "    for i in indices:\n",
    "        noisy_data[i] = np.random.choice([np.min(data), np.max(data)])\n",
    "    return noisy_data\n",
    "\n",
    "def add_noise(data):\n",
    "    noise_amp = 0.005 * np.random.uniform() * np.amax(data)\n",
    "    return data + noise_amp * np.random.normal(size=data.shape)\n",
    "\n",
    "def stretch(data, rate=0.8):\n",
    "    return librosa.effects.time_stretch(y=data, rate=rate)\n",
    "\n",
    "def pitch(data, sr, steps=4):\n",
    "    return librosa.effects.pitch_shift(y=data, sr=sr, n_steps=steps)\n",
    "\n",
    "\n",
    "# ========== Dataset ==========\n",
    "class EmotionAudioDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor_name=\"MIT/ast-finetuned-speech-commands-v2\", train=True):\n",
    "        df = dataframe.copy().dropna().reset_index(drop=True)\n",
    "\n",
    "        # Encode emotions to integers\n",
    "        self.label_map = {emotion: i for i, emotion in enumerate(sorted(df[\"Emotions\"].unique()))}\n",
    "        self.df = df.copy()\n",
    "        self.df[\"label\"] = self.df[\"Emotions\"].map(self.label_map)\n",
    "        \n",
    "        # Stratified split\n",
    "        train_df, val_df = train_test_split(self.df, test_size=0.2, stratify=self.df[\"label\"], random_state=42)\n",
    "\n",
    "        \n",
    "        self.df = train_df if train else val_df\n",
    "             # Step 2: Augment training set\n",
    "        augmented_rows_train = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            for aug_type in [\"original\", \"noise\", \"stretch_pitch\", \"salt_pepper\", \"pink\", \"clip\", \"volume\"]:\n",
    "                augmented_rows_train.append({\n",
    "                    \"Path\": row[\"Path\"],\n",
    "                    \"Emotions\": row[\"Emotions\"],\n",
    "                    \"label\": row[\"label\"],  # <-- ЭТО ВАЖНО\n",
    "                    \"AugType\": aug_type\n",
    "                })\n",
    "        self.df = pd.DataFrame(augmented_rows_train)\n",
    "\n",
    "        self.processor = AutoProcessor.from_pretrained(processor_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        file_path = row[\"Path\"]\n",
    "        label = row[\"label\"]\n",
    "        aug_type = row[\"AugType\"]\n",
    "\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading {file_path}: {e}\")\n",
    "            return torch.zeros((128, 128)), torch.tensor(0)\n",
    "        \n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0)\n",
    "        waveform = waveform.squeeze().numpy()\n",
    "\n",
    "        # Apply augmentation\n",
    "        if aug_type == \"noise\":\n",
    "            waveform = add_noise(waveform)\n",
    "        elif aug_type == \"stretch_pitch\":\n",
    "            waveform = stretch(waveform)\n",
    "            waveform = pitch(waveform, sample_rate)\n",
    "        elif aug_type == \"salt_pepper\":\n",
    "            waveform = salt_and_pepper_audio(waveform)\n",
    "        elif aug_type == \"pink\":\n",
    "            waveform = pink_noise(waveform)\n",
    "        elif aug_type == \"clip\":\n",
    "            waveform = clipping_distortion(waveform)\n",
    "        elif aug_type == \"volume\":\n",
    "            waveform = volume_perturbation(waveform)\n",
    "\n",
    "        # Resample if needed\n",
    "        if sample_rate != 16000:\n",
    "            waveform = librosa.resample(waveform, orig_sr=sample_rate, target_sr=16000)\n",
    "            sample_rate = 16000\n",
    "\n",
    "        # Back to tensor\n",
    "        waveform = torch.tensor(waveform)\n",
    "\n",
    "        try:\n",
    "            processed = self.processor(\n",
    "                waveform,\n",
    "                sampling_rate=16000,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                max_length=16000\n",
    "            )\n",
    "            input_values = processed.input_values.squeeze(0)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file_path}: {e}\")\n",
    "            return torch.zeros((128, 128)), torch.tensor(0)\n",
    "\n",
    "        return input_values, torch.tensor(label)\n",
    "\n",
    "# ========== Classifier ==========\n",
    "class CustomClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=768, num_classes=8):\n",
    "        super(CustomClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "\n",
    "class ComplexClassifierHead(nn.Module):\n",
    "    def __init__(self, input_dim=768, num_classes=8):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "\n",
    "        # Block 1\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.act1 = nn.GELU()\n",
    "        self.drop1 = nn.Dropout(0.3)\n",
    "\n",
    "        # Block 2\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.act2 = nn.GELU()\n",
    "        self.drop2 = nn.Dropout(0.3)\n",
    "\n",
    "          # Block 3\n",
    "        self.fc3 = nn.Linear(512,512)\n",
    "        self.act3 = nn.GELU()\n",
    "        self.drop3 = nn.Dropout(0.3)\n",
    "\n",
    "        # Block 4\n",
    "        self.fc4 = nn.Linear(512,128)\n",
    "        self.act4 = nn.GELU()\n",
    "        self.drop4 = nn.Dropout(0.3)\n",
    "\n",
    "        # Block 5\n",
    "        self.fc5 = nn.Linear(128, 128)\n",
    "        self.act5 = nn.GELU()\n",
    "        self.drop5 = nn.Dropout(0.3)\n",
    "\n",
    "        # Block 6\n",
    "        self.fc6 = nn.Linear(128, 128)\n",
    "        self.act6 = nn.GELU()\n",
    "        self.drop6 = nn.Dropout(0.3)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(128, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input normalization\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Block 1\n",
    "        x = self.drop1(self.act1(self.fc1(x)))\n",
    "        res1 = x\n",
    "\n",
    "        # Block 2\n",
    "        x = self.drop2(self.act2(self.fc2(x)))\n",
    "\n",
    "        # Block 3\n",
    "        x = self.drop3(self.act3(self.fc3(x)))\n",
    "        x = x + res1\n",
    "\n",
    "        # Block 4\n",
    "        x = self.drop4(self.act4(self.fc4(x)))\n",
    "        res2 = x\n",
    "\n",
    "        x = self.drop5(self.act5(self.fc5(x)))\n",
    "\n",
    "        x = self.drop6(self.act6(self.fc6(x)))\n",
    "        x = x + res2\n",
    "\n",
    "        # Output\n",
    "        return self.softmax(self.output(x))\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, path=\"checkpoints/checkpoint.pth\"):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, path)\n",
    "    print(f\"\\u2705 Checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "# ========== Training ==========\n",
    "def train_model(model, train_loader, val_loader, label_map, num_epochs=10, learning_rate=2e-5, checkpoint_path=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_labels = train_loader.dataset.df[\"label\"]\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(train_labels),\n",
    "        y=train_labels\n",
    "    )\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "\n",
    "    # log for saving full metrics\n",
    "    metrics_log = []\n",
    "\n",
    "    start_epoch = 0\n",
    "\n",
    "    ast_unfrozen = False\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        start_epoch = load_checkpoint(model, optimizer, checkpoint_path)\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        # === Unfreeze AST backbone after epoch 15 ===\n",
    "        if epoch == 5 and not ast_unfrozen:\n",
    "            print(\"🔓 Unfreezing AST backbone...\")\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "            ast_unfrozen = True\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        train_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "\n",
    "        for inputs, labels in train_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            train_bar.set_postfix(loss=loss.item(), acc=100 * correct / total)\n",
    "\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        val_bar = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_bar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(input_values=inputs).logits\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "                val_bar.set_postfix(loss=loss.item(), acc=100 * correct / total)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100 * correct / total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
    "        f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "        # Get full classification report\n",
    "        target_names = list(label_map.keys())\n",
    "        report = classification_report(\n",
    "            all_labels, all_preds,\n",
    "            target_names=target_names,\n",
    "            digits=4,\n",
    "            output_dict=True\n",
    "        )\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n",
    "        print(f\"F1 Score: {f1_weighted:.4f}\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n",
    "\n",
    "        save_checkpoint(model, optimizer, epoch + 1, path=f\"checkpoints/epoch_{epoch+1}.pth\")\n",
    "\n",
    "        # Flatten the classification report\n",
    "        flat_report = {\n",
    "            f\"{label}_{metric}\": round(value, 4)\n",
    "            for label, scores in report.items()\n",
    "            for metric, value in (scores.items() if isinstance(scores, dict) else [(\"value\", scores)])\n",
    "        }\n",
    "\n",
    "        # Save all metrics\n",
    "        epoch_metrics = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"train_loss\": round(train_loss, 4),\n",
    "            \"train_accuracy\": round(train_acc, 2),\n",
    "            \"val_loss\": round(val_loss, 4),\n",
    "            \"val_accuracy\": round(val_acc, 2),\n",
    "            \"f1_weighted\": round(f1_weighted, 4),\n",
    "            \"f1_macro\": round(f1_macro, 4),\n",
    "            \"report\": flat_report\n",
    "        }\n",
    "\n",
    "        metrics_log.append(epoch_metrics)\n",
    "        with open(\"metrics_log.json\", \"w\") as f:\n",
    "            json.dump(metrics_log, f, indent=2)\n",
    "\n",
    "    print(\"\\u2705 Training complete!\")\n",
    "\n",
    "    # Plot and save curves\n",
    "    total_epochs_ran = range(start_epoch + 1, start_epoch + len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(total_epochs_ran, train_losses, label='Train Loss')\n",
    "    plt.plot(total_epochs_ran, val_losses, label='Val Loss')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss Curve\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(total_epochs_ran, train_accs, label='Train Acc')\n",
    "    plt.plot(total_epochs_ran, val_accs, label='Val Acc')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy Curve\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"training_curves.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, path):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    print(f\"\\u2705 Loaded checkpoint from epoch {checkpoint['epoch']}\")\n",
    "    return checkpoint['epoch']\n",
    "\n",
    "# ========== Main ==========\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "\n",
    "    train_dataset = EmotionAudioDataset(data_path, train=True)\n",
    "    val_dataset = EmotionAudioDataset(data_path, train=False)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = ASTForAudioClassification.from_pretrained(\n",
    "        \"MIT/ast-finetuned-speech-commands-v2\",\n",
    "        num_labels=len(train_dataset.label_map),\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # custom_clf = CustomClassifier(model.config.hidden_size, len(train_dataset.label_map))\n",
    "    # model.classifier = custom_clf\n",
    "\n",
    "    custom_clf = ComplexClassifierHead(model.config.hidden_size, len(train_dataset.label_map))\n",
    "    model.classifier = custom_clf\n",
    "\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    label_map=train_dataset.label_map,\n",
    "    num_epochs=75, \n",
    "    checkpoint_path=\"checkpoints/epoch_52.pth\"\n",
    "    )\n",
    "\n",
    "    torch.save(model.state_dict(), \"final_model_emotions_with_augmentation_state_dict.pth\")\n",
    "    torch.save(model, \"final_model_emotions_with_augmentation.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d85aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcafddc-4a46-4be5-9eab-84284eab0026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3301ff12-7501-4ca2-9310-6cd9eecddab6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
