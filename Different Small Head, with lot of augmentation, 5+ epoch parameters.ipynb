{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4561d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchaudio\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import ASTForAudioClassification, AutoProcessor\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import json\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35d5d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ravdess = \"/mnt/c/Users/rubom/Desktop/Emotions DataSets/Ravdess/\"\n",
    "Crema   = \"/mnt/c/Users/rubom/Desktop/Emotions DataSets/Crema/AudioWAV/\"\n",
    "Tess    = \"/mnt/c/Users/rubom/Desktop/Emotions DataSets/Tess/TESS Toronto emotional speech set data/\"\n",
    "Savee   = \"/mnt/c/Users/rubom/Desktop/Emotions DataSets/Savee/ALL/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d6128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Emotions                                               Path\n",
      "0  neutral  /mnt/c/Users/rubom/Desktop/Emotions DataSets/R...\n",
      "1  neutral  /mnt/c/Users/rubom/Desktop/Emotions DataSets/R...\n",
      "2  neutral  /mnt/c/Users/rubom/Desktop/Emotions DataSets/R...\n",
      "3  neutral  /mnt/c/Users/rubom/Desktop/Emotions DataSets/R...\n",
      "4     calm  /mnt/c/Users/rubom/Desktop/Emotions DataSets/R...\n",
      "/mnt/c/Users/rubom/Desktop/Emotions DataSets/Ravdess/Actor_01/03-01-01-01-01-02-01.wav\n"
     ]
    }
   ],
   "source": [
    "ravdess_directory_list = [d for d in os.listdir(Ravdess) if d.startswith(\"Actor_\")]\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for dir in ravdess_directory_list:\n",
    "    actor_folder = os.path.join(Ravdess, dir)\n",
    "    \n",
    "    for file in os.listdir(actor_folder):\n",
    "        if not file.endswith('.wav'):\n",
    "            continue  \n",
    "        \n",
    "        part = file.split('.')[0].split('-')\n",
    "        \n",
    "        if len(part) != 7:\n",
    "            print(f\"‚ö†Ô∏è Skipping malformed file: {file}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            emotion_code = int(part[2])\n",
    "            file_emotion.append(emotion_code)\n",
    "            file_path.append(os.path.join(actor_folder, file))\n",
    "        except ValueError:\n",
    "            print(f\"‚ùå Couldn't parse emotion code in: {file}\")\n",
    "\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Ravdess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "\n",
    "emotion_labels = {\n",
    "    1: 'neutral',\n",
    "    2: 'calm',\n",
    "    3: 'happy',\n",
    "    4: 'sad',\n",
    "    5: 'angry',\n",
    "    6: 'fear',\n",
    "    7: 'disgust',\n",
    "    8: 'surprise'\n",
    "}\n",
    "Ravdess_df[\"Emotions\"] = Ravdess_df[\"Emotions\"].map(emotion_labels)\n",
    "\n",
    "print(Ravdess_df.head())\n",
    "print(Ravdess_df[\"Path\"].values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a913cea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/rubom/Desktop/Emotions DataSets/Crema/AudioWAV/1001_DFA_DIS_XX.wav\n"
     ]
    }
   ],
   "source": [
    "crema_directory_list = os.listdir(Crema)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for file in crema_directory_list:\n",
    "\n",
    "    file_path.append(Crema + file)\n",
    "\n",
    "    part=file.split('_')\n",
    "    if part[2] == 'SAD':\n",
    "        file_emotion.append('sad')\n",
    "    elif part[2] == 'ANG':\n",
    "        file_emotion.append('angry')\n",
    "    elif part[2] == 'DIS':\n",
    "        file_emotion.append('disgust')\n",
    "    elif part[2] == 'FEA':\n",
    "        file_emotion.append('fear')\n",
    "    elif part[2] == 'HAP':\n",
    "        file_emotion.append('happy')\n",
    "    elif part[2] == 'NEU':\n",
    "        file_emotion.append('neutral')\n",
    "    else:\n",
    "        file_emotion.append('Unknown')\n",
    "        \n",
    "\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Crema_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Crema_df.head()\n",
    "print(Crema_df[\"Path\"].values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed964004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/rubom/Desktop/Emotions DataSets/Tess/TESS Toronto emotional speech set data/OAF_angry/OAF_bar_angry.wav\n"
     ]
    }
   ],
   "source": [
    "tess_directory_list = os.listdir(Tess)\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for dir in tess_directory_list:\n",
    "    directories = os.listdir(Tess + \"/\"+dir)\n",
    "    for file in directories:\n",
    "        part = file.split('.')[0]\n",
    "        part = part.split('_')[2]\n",
    "\n",
    "        if part=='ps':\n",
    "            file_emotion.append('surprise')\n",
    "        else:\n",
    "            file_emotion.append(part)\n",
    "        file_path.append(Tess + dir + '/' + file)\n",
    "        \n",
    "\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Tess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Tess_df.head()\n",
    "print(Tess_df['Path'].values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720cdecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/rubom/Desktop/Emotions DataSets/Savee/ALL/DC_a02.wav\n"
     ]
    }
   ],
   "source": [
    "savee_directory_list = os.listdir(Savee)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for file in savee_directory_list:\n",
    "    file_path.append(Savee + file)\n",
    "    part = file.split('_')[1]\n",
    "    ele = part[:-6]\n",
    "    if ele=='a':\n",
    "        file_emotion.append('angry')\n",
    "    elif ele=='d':\n",
    "        file_emotion.append('disgust')\n",
    "    elif ele=='f':\n",
    "        file_emotion.append('fear')\n",
    "    elif ele=='h':\n",
    "        file_emotion.append('happy')\n",
    "    elif ele=='n':\n",
    "        file_emotion.append('neutral')\n",
    "    elif ele=='sa':\n",
    "        file_emotion.append('sad')\n",
    "    else:\n",
    "        file_emotion.append('surprise')\n",
    "        \n",
    "\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Savee_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Savee_df.head()\n",
    "print(Savee_df[\"Path\"].values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eb9bcea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>/mnt/c/Users/rubom/Desktop/Emotions DataSets/R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>/mnt/c/Users/rubom/Desktop/Emotions DataSets/R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>/mnt/c/Users/rubom/Desktop/Emotions DataSets/R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>/mnt/c/Users/rubom/Desktop/Emotions DataSets/R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>calm</td>\n",
       "      <td>/mnt/c/Users/rubom/Desktop/Emotions DataSets/R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>surprise</td>\n",
       "      <td>/mnt/c/Users/rubom/Desktop/Emotions DataSets/S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>surprise</td>\n",
       "      <td>/mnt/c/Users/rubom/Desktop/Emotions DataSets/S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>surprise</td>\n",
       "      <td>/mnt/c/Users/rubom/Desktop/Emotions DataSets/S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>surprise</td>\n",
       "      <td>/mnt/c/Users/rubom/Desktop/Emotions DataSets/S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>surprise</td>\n",
       "      <td>/mnt/c/Users/rubom/Desktop/Emotions DataSets/S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12162 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Emotions                                               Path\n",
       "0     neutral  /mnt/c/Users/rubom/Desktop/Emotions DataSets/R...\n",
       "1     neutral  /mnt/c/Users/rubom/Desktop/Emotions DataSets/R...\n",
       "2     neutral  /mnt/c/Users/rubom/Desktop/Emotions DataSets/R...\n",
       "3     neutral  /mnt/c/Users/rubom/Desktop/Emotions DataSets/R...\n",
       "4        calm  /mnt/c/Users/rubom/Desktop/Emotions DataSets/R...\n",
       "..        ...                                                ...\n",
       "475  surprise  /mnt/c/Users/rubom/Desktop/Emotions DataSets/S...\n",
       "476  surprise  /mnt/c/Users/rubom/Desktop/Emotions DataSets/S...\n",
       "477  surprise  /mnt/c/Users/rubom/Desktop/Emotions DataSets/S...\n",
       "478  surprise  /mnt/c/Users/rubom/Desktop/Emotions DataSets/S...\n",
       "479  surprise  /mnt/c/Users/rubom/Desktop/Emotions DataSets/S...\n",
       "\n",
       "[12162 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = pd.concat([Ravdess_df, Crema_df, Tess_df, Savee_df], axis = 0)\n",
    "data_path.to_csv(\"data_path.csv\",index=False)\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f09c6586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Count of Emotions', size=16)\n",
    "sns.countplot(data_path.Emotions)\n",
    "plt.ylabel('Count', size=12)\n",
    "plt.xlabel('Emotions', size=12)\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b86bb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-speech-commands-v2 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([35]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([35, 768]) in the checkpoint and torch.Size([8, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/75\n",
      "\n",
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.8948, Accuracy: 30.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.8552, Accuracy: 39.56%\n",
      "F1 Score: 0.3728\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.4508    0.5469    0.4942      2695\n",
      "        calm     0.1804    0.9474    0.3031       266\n",
      "     disgust     0.8624    0.0954    0.1717      2695\n",
      "        fear     0.4348    0.3525    0.3893      2695\n",
      "       happy     0.3640    0.2156    0.2708      2695\n",
      "     neutral     0.4074    0.4521    0.4286      2380\n",
      "         sad     0.3557    0.5573    0.4342      2695\n",
      "    surprise     0.4546    0.7099    0.5543       910\n",
      "\n",
      "    accuracy                         0.3956     17031\n",
      "   macro avg     0.4388    0.4846    0.3808     17031\n",
      "weighted avg     0.4745    0.3956    0.3728     17031\n",
      "\n",
      "‚úÖ Checkpoint saved at epoch 1\n",
      "\n",
      "Epoch 2/75\n",
      "\n",
      "Epoch 2/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 1.7899, Accuracy: 42.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.8032, Accuracy: 44.03%\n",
      "F1 Score: 0.4263\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.5381    0.6256    0.5786      2695\n",
      "        calm     0.1781    0.9887    0.3018       266\n",
      "     disgust     0.6442    0.1848    0.2872      2695\n",
      "        fear     0.5021    0.3169    0.3885      2695\n",
      "       happy     0.3872    0.2701    0.3183      2695\n",
      "     neutral     0.4266    0.5458    0.4789      2380\n",
      "         sad     0.4305    0.5399    0.4790      2695\n",
      "    surprise     0.4361    0.7868    0.5611       910\n",
      "\n",
      "    accuracy                         0.4403     17031\n",
      "   macro avg     0.4429    0.5323    0.4242     17031\n",
      "weighted avg     0.4816    0.4403    0.4263     17031\n",
      "\n",
      "‚úÖ Checkpoint saved at epoch 2\n",
      "\n",
      "Epoch 3/75\n",
      "\n",
      "Epoch 3/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 1.7526, Accuracy: 45.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.7804, Accuracy: 46.32%\n",
      "F1 Score: 0.4429\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.5326    0.7087    0.6082      2695\n",
      "        calm     0.2108    0.9699    0.3463       266\n",
      "     disgust     0.5629    0.2275    0.3240      2695\n",
      "        fear     0.5717    0.2694    0.3662      2695\n",
      "       happy     0.4220    0.2508    0.3146      2695\n",
      "     neutral     0.4665    0.5853    0.5192      2380\n",
      "         sad     0.4292    0.5870    0.4958      2695\n",
      "    surprise     0.4603    0.8033    0.5853       910\n",
      "\n",
      "    accuracy                         0.4632     17031\n",
      "   macro avg     0.4570    0.5502    0.4450     17031\n",
      "weighted avg     0.4916    0.4632    0.4429     17031\n",
      "\n",
      "‚úÖ Checkpoint saved at epoch 3\n",
      "\n",
      "Epoch 4/75\n",
      "\n",
      "Epoch 4/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 1.7326, Accuracy: 47.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.7679, Accuracy: 47.21%\n",
      "F1 Score: 0.4580\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.5956    0.6772    0.6338      2695\n",
      "        calm     0.2166    0.9812    0.3549       266\n",
      "     disgust     0.5694    0.2330    0.3307      2695\n",
      "        fear     0.5073    0.3210    0.3932      2695\n",
      "       happy     0.4076    0.2946    0.3420      2695\n",
      "     neutral     0.4502    0.6550    0.5336      2380\n",
      "         sad     0.5067    0.5035    0.5051      2695\n",
      "    surprise     0.4032    0.8264    0.5420       910\n",
      "\n",
      "    accuracy                         0.4721     17031\n",
      "   macro avg     0.4571    0.5615    0.4544     17031\n",
      "weighted avg     0.4972    0.4721    0.4580     17031\n",
      "\n",
      "‚úÖ Checkpoint saved at epoch 4\n",
      "\n",
      "Epoch 5/75\n",
      "\n",
      "Epoch 5/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 1.7173, Accuracy: 49.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.7558, Accuracy: 48.10%\n",
      "F1 Score: 0.4612\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.5454    0.7488    0.6311      2695\n",
      "        calm     0.2366    0.9624    0.3798       266\n",
      "     disgust     0.5566    0.2571    0.3518      2695\n",
      "        fear     0.5146    0.3147    0.3905      2695\n",
      "       happy     0.4357    0.2401    0.3096      2695\n",
      "     neutral     0.5062    0.5878    0.5439      2380\n",
      "         sad     0.4794    0.5833    0.5263      2695\n",
      "    surprise     0.4152    0.8341    0.5544       910\n",
      "\n",
      "    accuracy                         0.4810     17031\n",
      "   macro avg     0.4612    0.5660    0.4609     17031\n",
      "weighted avg     0.4972    0.4810    0.4612     17031\n",
      "\n",
      "‚úÖ Checkpoint saved at epoch 5\n",
      "\n",
      "Epoch 6/75\n",
      "üîì Unfreezing AST backbone...\n",
      "\n",
      "Epoch 6/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 1.6240, Accuracy: 59.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6571, Accuracy: 58.58%\n",
      "F1 Score: 0.5875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.7260    0.7336    0.7298      2695\n",
      "        calm     0.2387    0.9737    0.3834       266\n",
      "     disgust     0.5629    0.4835    0.5202      2695\n",
      "        fear     0.5933    0.5109    0.5490      2695\n",
      "       happy     0.5304    0.4438    0.4832      2695\n",
      "     neutral     0.6490    0.6424    0.6457      2380\n",
      "         sad     0.5971    0.6093    0.6031      2695\n",
      "    surprise     0.5661    0.7626    0.6498       910\n",
      "\n",
      "    accuracy                         0.5858     17031\n",
      "   macro avg     0.5579    0.6450    0.5705     17031\n",
      "weighted avg     0.6009    0.5858    0.5875     17031\n",
      "\n",
      "‚úÖ Checkpoint saved at epoch 6\n",
      "\n",
      "Epoch 7/75\n",
      "\n",
      "Epoch 7/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 1.5234, Accuracy: 71.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6370, Accuracy: 60.77%\n",
      "F1 Score: 0.6061\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.6775    0.7911    0.7299      2695\n",
      "        calm     0.2989    0.9248    0.4518       266\n",
      "     disgust     0.6440    0.4638    0.5393      2695\n",
      "        fear     0.5265    0.5796    0.5517      2695\n",
      "       happy     0.6333    0.4935    0.5547      2695\n",
      "     neutral     0.6836    0.6055    0.6422      2380\n",
      "         sad     0.5771    0.6071    0.5917      2695\n",
      "    surprise     0.6775    0.8264    0.7446       910\n",
      "\n",
      "    accuracy                         0.6077     17031\n",
      "   macro avg     0.5898    0.6615    0.6007     17031\n",
      "weighted avg     0.6203    0.6077    0.6061     17031\n",
      "\n",
      "‚úÖ Checkpoint saved at epoch 7\n",
      "\n",
      "Epoch 8/75\n",
      "\n",
      "Epoch 8/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 1.4650, Accuracy: 77.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6259, Accuracy: 61.78%\n",
      "F1 Score: 0.6188\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.7623    0.7139    0.7373      2695\n",
      "        calm     0.3029    0.9474    0.4590       266\n",
      "     disgust     0.5807    0.5232    0.5505      2695\n",
      "        fear     0.5895    0.5009    0.5416      2695\n",
      "       happy     0.6264    0.5644    0.5938      2695\n",
      "     neutral     0.6754    0.6391    0.6567      2380\n",
      "         sad     0.5668    0.6631    0.6111      2695\n",
      "    surprise     0.6726    0.8308    0.7434       910\n",
      "\n",
      "    accuracy                         0.6178     17031\n",
      "   macro avg     0.5971    0.6728    0.6117     17031\n",
      "weighted avg     0.6297    0.6178    0.6188     17031\n",
      "\n",
      "‚úÖ Checkpoint saved at epoch 8\n",
      "\n",
      "Epoch 9/75\n",
      "\n",
      "Epoch 9/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 1.4359, Accuracy: 81.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6270, Accuracy: 61.89%\n",
      "F1 Score: 0.6140\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.7059    0.7803    0.7413      2695\n",
      "        calm     0.3026    0.9248    0.4560       266\n",
      "     disgust     0.5908    0.5154    0.5505      2695\n",
      "        fear     0.7176    0.3763    0.4937      2695\n",
      "       happy     0.6501    0.5688    0.6068      2695\n",
      "     neutral     0.6270    0.6920    0.6579      2380\n",
      "         sad     0.5498    0.6820    0.6088      2695\n",
      "    surprise     0.6722    0.8473    0.7496       910\n",
      "\n",
      "    accuracy                         0.6189     17031\n",
      "   macro avg     0.6020    0.6734    0.6081     17031\n",
      "weighted avg     0.6369    0.6189    0.6140     17031\n",
      "\n",
      "‚úÖ Checkpoint saved at epoch 9\n",
      "\n",
      "Epoch 10/75\n",
      "\n",
      "Epoch 10/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1.4170, Accuracy: 83.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6272, Accuracy: 61.85%\n",
      "F1 Score: 0.6181\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.7741    0.7121    0.7418      2695\n",
      "        calm     0.3177    0.9436    0.4754       266\n",
      "     disgust     0.5458    0.5510    0.5484      2695\n",
      "        fear     0.6561    0.4078    0.5030      2695\n",
      "       happy     0.6198    0.6193    0.6195      2695\n",
      "     neutral     0.6951    0.6408    0.6668      2380\n",
      "         sad     0.5351    0.6783    0.5983      2695\n",
      "    surprise     0.7131    0.8330    0.7684       910\n",
      "\n",
      "    accuracy                         0.6185     17031\n",
      "   macro avg     0.6071    0.6732    0.6152     17031\n",
      "weighted avg     0.6356    0.6185    0.6181     17031\n",
      "\n",
      "‚úÖ Checkpoint saved at epoch 10\n",
      "\n",
      "Epoch 11/75\n",
      "\n",
      "Epoch 11/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 1.4026, Accuracy: 85.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6258, Accuracy: 62.16%\n",
      "F1 Score: 0.6217\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.7852    0.7054    0.7432      2695\n",
      "        calm     0.3261    0.9023    0.4790       266\n",
      "     disgust     0.6012    0.4950    0.5429      2695\n",
      "        fear     0.6071    0.5058    0.5518      2695\n",
      "       happy     0.5844    0.6330    0.6078      2695\n",
      "     neutral     0.6108    0.7109    0.6571      2380\n",
      "         sad     0.5933    0.5937    0.5935      2695\n",
      "    surprise     0.7324    0.8242    0.7756       910\n",
      "\n",
      "    accuracy                         0.6216     17031\n",
      "   macro avg     0.6051    0.6713    0.6189     17031\n",
      "weighted avg     0.6314    0.6216    0.6217     17031\n",
      "\n",
      "‚úÖ Checkpoint saved at epoch 11\n",
      "\n",
      "Epoch 12/75\n",
      "\n",
      "Epoch 12/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 1.3922, Accuracy: 86.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6225, Accuracy: 62.64%\n",
      "F1 Score: 0.6217\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.6859    0.7818    0.7307      2695\n",
      "        calm     0.3398    0.9286    0.4975       266\n",
      "     disgust     0.6334    0.5128    0.5667      2695\n",
      "        fear     0.6488    0.4134    0.5050      2695\n",
      "       happy     0.6098    0.6100    0.6099      2695\n",
      "     neutral     0.6049    0.7508    0.6700      2380\n",
      "         sad     0.6086    0.6163    0.6125      2695\n",
      "    surprise     0.7610    0.7978    0.7790       910\n",
      "\n",
      "    accuracy                         0.6264     17031\n",
      "   macro avg     0.6115    0.6764    0.6214     17031\n",
      "weighted avg     0.6347    0.6264    0.6217     17031\n",
      "\n",
      "‚úÖ Checkpoint saved at epoch 12\n",
      "\n",
      "Epoch 13/75\n",
      "\n",
      "Epoch 13/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 1.3827, Accuracy: 87.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6133, Accuracy: 63.34%\n",
      "F1 Score: 0.6340\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.7322    0.7729    0.7520      2695\n",
      "        calm     0.3143    0.9511    0.4725       266\n",
      "     disgust     0.6174    0.5180    0.5634      2695\n",
      "        fear     0.5579    0.5525    0.5552      2695\n",
      "       happy     0.6903    0.5466    0.6101      2695\n",
      "     neutral     0.6943    0.6748    0.6844      2380\n",
      "         sad     0.5791    0.6453    0.6104      2695\n",
      "    surprise     0.7483    0.8231    0.7839       910\n",
      "\n",
      "    accuracy                         0.6334     17031\n",
      "   macro avg     0.6167    0.6855    0.6290     17031\n",
      "weighted avg     0.6446    0.6334    0.6340     17031\n",
      "\n",
      "‚úÖ Checkpoint saved at epoch 13\n",
      "\n",
      "Epoch 14/75\n",
      "\n",
      "Epoch 14/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 1.3773, Accuracy: 88.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6184, Accuracy: 63.20%\n",
      "F1 Score: 0.6311\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.7203    0.7770    0.7476      2695\n",
      "        calm     0.2869    0.8985    0.4349       266\n",
      "     disgust     0.5644    0.5640    0.5642      2695\n",
      "        fear     0.6624    0.4638    0.5456      2695\n",
      "       happy     0.6773    0.5607    0.6135      2695\n",
      "     neutral     0.6421    0.7403    0.6877      2380\n",
      "         sad     0.6089    0.6067    0.6078      2695\n",
      "    surprise     0.7165    0.8275    0.7680       910\n",
      "\n",
      "    accuracy                         0.6320     17031\n",
      "   macro avg     0.6099    0.6798    0.6212     17031\n",
      "weighted avg     0.6442    0.6320    0.6311     17031\n",
      "\n",
      "‚úÖ Checkpoint saved at epoch 14\n",
      "\n",
      "Epoch 15/75\n",
      "\n",
      "Epoch 15/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 1.3698, Accuracy: 88.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6161, Accuracy: 63.82%\n",
      "F1 Score: 0.6375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.7285    0.7826    0.7546      2695\n",
      "        calm     0.3555    0.8459    0.5006       266\n",
      "     disgust     0.6151    0.5195    0.5633      2695\n",
      "        fear     0.5632    0.5737    0.5684      2695\n",
      "       happy     0.7221    0.5514    0.6253      2695\n",
      "     neutral     0.6448    0.7391    0.6887      2380\n",
      "         sad     0.5885    0.6059    0.5971      2695\n",
      "    surprise     0.7720    0.7813    0.7766       910\n",
      "\n",
      "    accuracy                         0.6382     17031\n",
      "   macro avg     0.6237    0.6749    0.6343     17031\n",
      "weighted avg     0.6460    0.6382    0.6375     17031\n",
      "\n",
      "‚úÖ Checkpoint saved at epoch 15\n",
      "\n",
      "Epoch 16/75\n",
      "\n",
      "Epoch 16/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 1.3654, Accuracy: 89.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6216, Accuracy: 63.40%\n",
      "F1 Score: 0.6346\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.7736    0.6909    0.7299      2695\n",
      "        calm     0.3800    0.7556    0.5057       266\n",
      "     disgust     0.5913    0.5685    0.5796      2695\n",
      "        fear     0.5954    0.5046    0.5463      2695\n",
      "       happy     0.5877    0.6712    0.6267      2695\n",
      "     neutral     0.6958    0.6630    0.6790      2380\n",
      "         sad     0.5919    0.6360    0.6131      2695\n",
      "    surprise     0.7577    0.8143    0.7850       910\n",
      "\n",
      "    accuracy                         0.6340     17031\n",
      "   macro avg     0.6217    0.6630    0.6332     17031\n",
      "weighted avg     0.6405    0.6340    0.6346     17031\n",
      "\n",
      "‚úÖ Checkpoint saved at epoch 16\n",
      "\n",
      "Epoch 17/75\n",
      "\n",
      "Epoch 17/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 1.3600, Accuracy: 90.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6243, Accuracy: 63.01%\n",
      "F1 Score: 0.6288\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.6998    0.7603    0.7288      2695\n",
      "        calm     0.3559    0.8496    0.5017       266\n",
      "     disgust     0.6099    0.5250    0.5643      2695\n",
      "        fear     0.5943    0.4887    0.5363      2695\n",
      "       happy     0.6154    0.6122    0.6138      2695\n",
      "     neutral     0.6619    0.7050    0.6828      2380\n",
      "         sad     0.5942    0.6271    0.6102      2695\n",
      "    surprise     0.8108    0.7769    0.7935       910\n",
      "\n",
      "    accuracy                         0.6301     17031\n",
      "   macro avg     0.6178    0.6681    0.6289     17031\n",
      "weighted avg     0.6341    0.6301    0.6288     17031\n",
      "\n",
      "‚úÖ Checkpoint saved at epoch 17\n",
      "\n",
      "Epoch 18/75\n",
      "\n",
      "Epoch 18/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: 1.3558, Accuracy: 90.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6231, Accuracy: 63.04%\n",
      "F1 Score: 0.6285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.7143    0.7599    0.7364      2695\n",
      "        calm     0.4040    0.7519    0.5256       266\n",
      "     disgust     0.5651    0.5588    0.5619      2695\n",
      "        fear     0.5984    0.5065    0.5486      2695\n",
      "       happy     0.6248    0.6141    0.6194      2695\n",
      "     neutral     0.6823    0.6903    0.6863      2380\n",
      "         sad     0.6111    0.5744    0.5922      2695\n",
      "    surprise     0.6814    0.8484    0.7558       910\n",
      "\n",
      "    accuracy                         0.6304     17031\n",
      "   macro avg     0.6102    0.6630    0.6283     17031\n",
      "weighted avg     0.6308    0.6304    0.6285     17031\n",
      "\n",
      "‚úÖ Checkpoint saved at epoch 18\n",
      "\n",
      "Epoch 19/75\n",
      "\n",
      "Epoch 19/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: 1.3522, Accuracy: 91.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6325, Accuracy: 62.11%\n",
      "F1 Score: 0.6183\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.6579    0.7907    0.7182      2695\n",
      "        calm     0.3724    0.7519    0.4981       266\n",
      "     disgust     0.6225    0.5250    0.5696      2695\n",
      "        fear     0.5784    0.5199    0.5476      2695\n",
      "       happy     0.6021    0.5896    0.5958      2695\n",
      "     neutral     0.6639    0.6655    0.6647      2380\n",
      "         sad     0.6261    0.5592    0.5907      2695\n",
      "    surprise     0.6658    0.8253    0.7370       910\n",
      "\n",
      "    accuracy                         0.6211     17031\n",
      "   macro avg     0.5986    0.6534    0.6152     17031\n",
      "weighted avg     0.6227    0.6211    0.6183     17031\n",
      "\n",
      "‚úÖ Checkpoint saved at epoch 19\n",
      "\n",
      "Epoch 20/75\n",
      "\n",
      "Epoch 20/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 1.3505, Accuracy: 91.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6337, Accuracy: 62.56%\n",
      "F1 Score: 0.6248\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.7041    0.7629    0.7323      2695\n",
      "        calm     0.3975    0.7143    0.5108       266\n",
      "     disgust     0.5926    0.5321    0.5607      2695\n",
      "        fear     0.5912    0.5161    0.5511      2695\n",
      "       happy     0.5868    0.5892    0.5880      2695\n",
      "     neutral     0.6548    0.7055    0.6792      2380\n",
      "         sad     0.5875    0.6119    0.5994      2695\n",
      "    surprise     0.8531    0.7341    0.7891       910\n",
      "\n",
      "    accuracy                         0.6256     17031\n",
      "   macro avg     0.6209    0.6458    0.6263     17031\n",
      "weighted avg     0.6279    0.6256    0.6248     17031\n",
      "\n",
      "‚úÖ Checkpoint saved at epoch 20\n",
      "\n",
      "Epoch 21/75\n",
      "\n",
      "Epoch 21/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Loss: 1.3466, Accuracy: 91.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6260, Accuracy: 62.87%\n",
      "F1 Score: 0.6276\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.7466    0.7432    0.7449      2695\n",
      "        calm     0.3660    0.8008    0.5024       266\n",
      "     disgust     0.5388    0.6137    0.5738      2695\n",
      "        fear     0.6055    0.4612    0.5236      2695\n",
      "       happy     0.6902    0.5506    0.6126      2695\n",
      "     neutral     0.6340    0.7315    0.6793      2380\n",
      "         sad     0.5869    0.6067    0.5966      2695\n",
      "    surprise     0.7638    0.8066    0.7846       910\n",
      "\n",
      "    accuracy                         0.6287     17031\n",
      "   macro avg     0.6165    0.6643    0.6272     17031\n",
      "weighted avg     0.6364    0.6287    0.6276     17031\n",
      "\n",
      "‚úÖ Checkpoint saved at epoch 21\n",
      "\n",
      "Epoch 22/75\n",
      "\n",
      "Epoch 22/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Loss: 1.3455, Accuracy: 91.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6223, Accuracy: 63.48%\n",
      "F1 Score: 0.6340\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.7701    0.7009    0.7339      2695\n",
      "        calm     0.3735    0.7105    0.4896       266\n",
      "     disgust     0.6151    0.5573    0.5848      2695\n",
      "        fear     0.6129    0.4994    0.5504      2695\n",
      "       happy     0.5623    0.6887    0.6191      2695\n",
      "     neutral     0.6400    0.7298    0.6820      2380\n",
      "         sad     0.6433    0.5729    0.6061      2695\n",
      "    surprise     0.7341    0.8220    0.7755       910\n",
      "\n",
      "    accuracy                         0.6348     17031\n",
      "   macro avg     0.6189    0.6602    0.6302     17031\n",
      "weighted avg     0.6414    0.6348    0.6340     17031\n",
      "\n",
      "‚úÖ Checkpoint saved at epoch 22\n",
      "\n",
      "Epoch 23/75\n",
      "\n",
      "Epoch 23/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 415\u001b[39m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model.classifier.parameters():\n\u001b[32m    413\u001b[39m     param.requires_grad = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m\u001b[49m\u001b[43mlabel_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlabel_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# total number of epochs you want to train for\u001b[39;49;00m\n\u001b[32m    421\u001b[39m \u001b[43m\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcheckpoints/epoch_52.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    422\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    424\u001b[39m torch.save(model.state_dict(), \u001b[33m\"\u001b[39m\u001b[33mfinal_model_emotions_with_augmentation_state_dict.pth\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    425\u001b[39m torch.save(model, \u001b[33m\"\u001b[39m\u001b[33mfinal_model_emotions_with_augmentation.pth\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 256\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, label_map, num_epochs, learning_rate, checkpoint_path)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    254\u001b[39m train_bar = tqdm(train_loader, desc=\u001b[33m\"\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_bar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf/lib/python3.11/site-packages/torch/utils/data/dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 112\u001b[39m, in \u001b[36mEmotionAudioDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    110\u001b[39m     waveform = add_noise(waveform)\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m aug_type == \u001b[33m\"\u001b[39m\u001b[33mstretch_pitch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     waveform = \u001b[43mstretch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m     waveform = pitch(waveform, sample_rate)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m aug_type == \u001b[33m\"\u001b[39m\u001b[33msalt_pepper\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mstretch\u001b[39m\u001b[34m(data, rate)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstretch\u001b[39m(data, rate=\u001b[32m0.8\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlibrosa\u001b[49m\u001b[43m.\u001b[49m\u001b[43meffects\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtime_stretch\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrate\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf/lib/python3.11/site-packages/librosa/effects.py:383\u001b[39m, in \u001b[36mtime_stretch\u001b[39m\u001b[34m(y, rate, **kwargs)\u001b[39m\n\u001b[32m    380\u001b[39m stft = core.stft(y, **kwargs)\n\u001b[32m    382\u001b[39m \u001b[38;5;66;03m# Stretch by phase vocoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m stft_stretch = \u001b[43mcore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mphase_vocoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhop_length\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn_fft\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[38;5;66;03m# Predict the length of y_stretch\u001b[39;00m\n\u001b[32m    391\u001b[39m len_stretch = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(y.shape[-\u001b[32m1\u001b[39m] / rate))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf/lib/python3.11/site-packages/librosa/core/spectrum.py:1465\u001b[39m, in \u001b[36mphase_vocoder\u001b[39m\u001b[34m(D, rate, hop_length, n_fft)\u001b[39m\n\u001b[32m   1462\u001b[39m d_stretch[..., t] = util.phasor(phase_acc, mag=mag)\n\u001b[32m   1464\u001b[39m \u001b[38;5;66;03m# Compute phase advance\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1465\u001b[39m dphase = np.angle(columns[..., \u001b[32m1\u001b[39m]) - \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mangle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m - phi_advance\n\u001b[32m   1467\u001b[39m \u001b[38;5;66;03m# Wrap to -pi:pi range\u001b[39;00m\n\u001b[32m   1468\u001b[39m dphase = dphase - \u001b[32m2.0\u001b[39m * np.pi * np.round(dphase / (\u001b[32m2.0\u001b[39m * np.pi))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1697\u001b[39m, in \u001b[36mangle\u001b[39m\u001b[34m(z, deg)\u001b[39m\n\u001b[32m   1694\u001b[39m     zimag = \u001b[32m0\u001b[39m\n\u001b[32m   1695\u001b[39m     zreal = z\n\u001b[32m-> \u001b[39m\u001b[32m1697\u001b[39m a = \u001b[43marctan2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzimag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzreal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m deg:\n\u001b[32m   1699\u001b[39m     a *= \u001b[32m180\u001b[39m/pi\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def volume_perturbation(data, low=0.5, high=1.5):\n",
    "    factor = np.random.uniform(low, high)\n",
    "    return data * factor\n",
    "\n",
    "def clipping_distortion(data, clip_percent=0.02):\n",
    "    threshold = clip_percent * np.max(np.abs(data))\n",
    "    data = np.clip(data, -threshold, threshold)\n",
    "    return data\n",
    "\n",
    "def pink_noise(data):\n",
    "\n",
    "    uneven = data.shape[0] % 2\n",
    "    X = np.random.randn(data.shape[0]//2 + 1 + uneven) + 1j * np.random.randn(data.shape[0]//2 + 1 + uneven)\n",
    "    S = np.sqrt(np.arange(len(X)) + 1.)  \n",
    "    y = (np.fft.irfft(X / S)).real\n",
    "    if uneven:\n",
    "        y = y[:-1]\n",
    "    y = y / np.max(np.abs(y)) \n",
    "    return data + 0.005 * y\n",
    "\n",
    "def salt_and_pepper_audio(data, amount=0.005):\n",
    "    noisy_data = data.copy()\n",
    "    num_samples = int(amount * data.shape[0])\n",
    "    indices = np.random.randint(0, data.shape[0], num_samples)\n",
    "    for i in indices:\n",
    "        noisy_data[i] = np.random.choice([np.min(data), np.max(data)])\n",
    "    return noisy_data\n",
    "\n",
    "def add_noise(data):\n",
    "    noise_amp = 0.005 * np.random.uniform() * np.amax(data)\n",
    "    return data + noise_amp * np.random.normal(size=data.shape)\n",
    "\n",
    "def stretch(data, rate=0.8):\n",
    "    return librosa.effects.time_stretch(y=data, rate=rate)\n",
    "\n",
    "def pitch(data, sr, steps=4):\n",
    "    return librosa.effects.pitch_shift(y=data, sr=sr, n_steps=steps)\n",
    "\n",
    "\n",
    "# ========== Dataset ==========\n",
    "class EmotionAudioDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor_name=\"MIT/ast-finetuned-speech-commands-v2\", train=True):\n",
    "        df = dataframe.copy().dropna().reset_index(drop=True)\n",
    "\n",
    "        # Encode emotions to integers\n",
    "        self.label_map = {emotion: i for i, emotion in enumerate(sorted(df[\"Emotions\"].unique()))}\n",
    "        self.df = df.copy()\n",
    "        self.df[\"label\"] = self.df[\"Emotions\"].map(self.label_map)\n",
    "        \n",
    "        # Stratified split\n",
    "        train_df, val_df = train_test_split(self.df, test_size=0.2, stratify=self.df[\"label\"], random_state=42)\n",
    "\n",
    "        \n",
    "        self.df = train_df if train else val_df\n",
    "             # Step 2: Augment training set\n",
    "        augmented_rows_train = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            for aug_type in [\"original\", \"noise\", \"stretch_pitch\", \"salt_pepper\", \"pink\", \"clip\", \"volume\"]:\n",
    "                augmented_rows_train.append({\n",
    "                    \"Path\": row[\"Path\"],\n",
    "                    \"Emotions\": row[\"Emotions\"],\n",
    "                    \"label\": row[\"label\"],  # <-- –≠–¢–û –í–ê–ñ–ù–û\n",
    "                    \"AugType\": aug_type\n",
    "                })\n",
    "        self.df = pd.DataFrame(augmented_rows_train)\n",
    "\n",
    "        self.processor = AutoProcessor.from_pretrained(processor_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        file_path = row[\"Path\"]\n",
    "        label = row[\"label\"]\n",
    "        aug_type = row[\"AugType\"]\n",
    "\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {file_path}: {e}\")\n",
    "            return torch.zeros((128, 128)), torch.tensor(0)\n",
    "        \n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0)\n",
    "        waveform = waveform.squeeze().numpy()\n",
    "\n",
    "        # Apply augmentation\n",
    "        if aug_type == \"noise\":\n",
    "            waveform = add_noise(waveform)\n",
    "        elif aug_type == \"stretch_pitch\":\n",
    "            waveform = stretch(waveform)\n",
    "            waveform = pitch(waveform, sample_rate)\n",
    "        elif aug_type == \"salt_pepper\":\n",
    "            waveform = salt_and_pepper_audio(waveform)\n",
    "        elif aug_type == \"pink\":\n",
    "            waveform = pink_noise(waveform)\n",
    "        elif aug_type == \"clip\":\n",
    "            waveform = clipping_distortion(waveform)\n",
    "        elif aug_type == \"volume\":\n",
    "            waveform = volume_perturbation(waveform)\n",
    "\n",
    "        # Resample if needed\n",
    "        if sample_rate != 16000:\n",
    "            waveform = librosa.resample(waveform, orig_sr=sample_rate, target_sr=16000)\n",
    "            sample_rate = 16000\n",
    "\n",
    "        # Back to tensor\n",
    "        waveform = torch.tensor(waveform)\n",
    "\n",
    "        try:\n",
    "            processed = self.processor(\n",
    "                waveform,\n",
    "                sampling_rate=16000,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                max_length=16000\n",
    "            )\n",
    "            input_values = processed.input_values.squeeze(0)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {file_path}: {e}\")\n",
    "            return torch.zeros((128, 128)), torch.tensor(0)\n",
    "\n",
    "        return input_values, torch.tensor(label)\n",
    "\n",
    "# ========== Classifier ==========\n",
    "class CustomClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(CustomClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)\n",
    "    \n",
    "\n",
    "class ComplexClassifierHead(nn.Module):\n",
    "    def __init__(self, input_dim=768, num_classes=8):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.act1 = nn.GELU()\n",
    "        self.drop1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.act2 = nn.GELU()\n",
    "        self.drop2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.res_fc = nn.Linear(input_dim, 512)  # for residual connection\n",
    "\n",
    "        self.output = nn.Linear(512, num_classes)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = [B, D] where D=768\n",
    "        x = self.norm(x)\n",
    "\n",
    "        res = self.res_fc(x)  # residual path\n",
    "\n",
    "        x = self.drop1(self.act1(self.fc1(x)))\n",
    "        x = self.drop2(self.act2(self.fc2(x)))\n",
    "\n",
    "        x = x + res  # add residual\n",
    "\n",
    "        return self.softmax(self.output(x))\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, path=\"checkpoints/checkpoint.pth\"):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, path)\n",
    "    print(f\"\\u2705 Checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "# ========== Training ==========\n",
    "def train_model(model, train_loader, val_loader, label_map, num_epochs=10, learning_rate=2e-5, checkpoint_path=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_labels = train_loader.dataset.df[\"label\"]\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(train_labels),\n",
    "        y=train_labels\n",
    "    )\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "\n",
    "    # log for saving full metrics\n",
    "    metrics_log = []\n",
    "\n",
    "    start_epoch = 0\n",
    "\n",
    "    ast_unfrozen = False\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        start_epoch = load_checkpoint(model, optimizer, checkpoint_path)\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        # === Unfreeze AST backbone after epoch 15 ===\n",
    "        if epoch == 5 and not ast_unfrozen:\n",
    "            print(\"üîì Unfreezing AST backbone...\")\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "            ast_unfrozen = True\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        train_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "\n",
    "        for inputs, labels in train_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            train_bar.set_postfix(loss=loss.item(), acc=100 * correct / total)\n",
    "\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        val_bar = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_bar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(input_values=inputs).logits\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "                val_bar.set_postfix(loss=loss.item(), acc=100 * correct / total)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100 * correct / total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
    "        f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "        # Get full classification report\n",
    "        target_names = list(label_map.keys())\n",
    "        report = classification_report(\n",
    "            all_labels, all_preds,\n",
    "            target_names=target_names,\n",
    "            digits=4,\n",
    "            output_dict=True\n",
    "        )\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n",
    "        print(f\"F1 Score: {f1_weighted:.4f}\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n",
    "\n",
    "        save_checkpoint(model, optimizer, epoch + 1, path=f\"checkpoints/epoch_{epoch+1}.pth\")\n",
    "\n",
    "        # Flatten the classification report\n",
    "        flat_report = {\n",
    "            f\"{label}_{metric}\": round(value, 4)\n",
    "            for label, scores in report.items()\n",
    "            for metric, value in (scores.items() if isinstance(scores, dict) else [(\"value\", scores)])\n",
    "        }\n",
    "\n",
    "        # Save all metrics\n",
    "        epoch_metrics = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"train_loss\": round(train_loss, 4),\n",
    "            \"train_accuracy\": round(train_acc, 2),\n",
    "            \"val_loss\": round(val_loss, 4),\n",
    "            \"val_accuracy\": round(val_acc, 2),\n",
    "            \"f1_weighted\": round(f1_weighted, 4),\n",
    "            \"f1_macro\": round(f1_macro, 4),\n",
    "            \"report\": flat_report\n",
    "        }\n",
    "\n",
    "        metrics_log.append(epoch_metrics)\n",
    "        with open(\"metrics_log.json\", \"w\") as f:\n",
    "            json.dump(metrics_log, f, indent=2)\n",
    "\n",
    "    print(\"\\u2705 Training complete!\")\n",
    "\n",
    "    # Plot and save curves\n",
    "    total_epochs_ran = range(start_epoch + 1, start_epoch + len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(total_epochs_ran, train_losses, label='Train Loss')\n",
    "    plt.plot(total_epochs_ran, val_losses, label='Val Loss')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss Curve\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(total_epochs_ran, train_accs, label='Train Acc')\n",
    "    plt.plot(total_epochs_ran, val_accs, label='Val Acc')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy Curve\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"training_curves.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, path):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    print(f\"\\u2705 Loaded checkpoint from epoch {checkpoint['epoch']}\")\n",
    "    return checkpoint['epoch']\n",
    "\n",
    "# ========== Main ==========\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "\n",
    "    train_dataset = EmotionAudioDataset(data_path, train=True)\n",
    "    val_dataset = EmotionAudioDataset(data_path, train=False)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = ASTForAudioClassification.from_pretrained(\n",
    "        \"MIT/ast-finetuned-speech-commands-v2\",\n",
    "        num_labels=len(train_dataset.label_map),\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # custom_clf = CustomClassifier(model.config.hidden_size, len(train_dataset.label_map))\n",
    "    # model.classifier = custom_clf\n",
    "\n",
    "    custom_clf = ComplexClassifierHead(model.config.hidden_size, len(train_dataset.label_map))\n",
    "    model.classifier = custom_clf\n",
    "\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    label_map=train_dataset.label_map,\n",
    "    num_epochs=75, \n",
    "    checkpoint_path=\"checkpoints/epoch_52.pth\"\n",
    "    )\n",
    "\n",
    "    torch.save(model.state_dict(), \"final_model_emotions_with_augmentation_state_dict.pth\")\n",
    "    torch.save(model, \"final_model_emotions_with_augmentation.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d85aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcafddc-4a46-4be5-9eab-84284eab0026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3301ff12-7501-4ca2-9310-6cd9eecddab6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
